**Optical Music Recognition** (OMR) is the task of getting a computer to read and understand written or printed sheet music in traditional Western notation and extract the encoded musical information. A successful OMR program has numerous applications in musicology and music performance, including but not limited to enabling permanent preservation of an enormous percentage of the Western musical canon that is impossible to digitize due to the manual labor required to do so; and allowing for composers to work on paper and easily transfer their work over to formats suited for modern printing requirements.

Intuitively, OMR seems like a relatively simple problem which can be solved through classical computer vision techniques. That is, one could imagine that we could match areas of the image against predefined musical symbols and algorithmically put them together to reconstruct the musical information encoded by the sheet music, also called a **score**. As such, traditional pipelines in OMR research have relied on multi-step algorithmic approaches, which would look something like this: 1) separate marked symbols in (black, ink) from the background (white, paper). 2) remove staff lines and other guiding symbols that provide no musical information. 3) perform template matching on the remaining symbols to classify them against a library of musical notation symbols. 4) use the relative location of said symbols to decipher the musical meaning encoded by the sheet music. 

However, research in OMR has to-date been largely unsuccessful in creating systems that can perform reliably on sheet music of any meaningful complexity. This is due to several subtle difficulties that make the above approach unreliable: 1) there is an extremely vast library of possible symbols in Western music notation, many of which look very similar (for example, a staccato dot versus a quarter note head, which are both represented by a round black dot). 2) the semantic meaning of individual objects is often context-dependent. That is, one symbol can mean drastically different things depending on its location relative to other symbols on the score; see the first figure below. 3) the frequent rule-breaking in classical music notation makes it nearly impossible to reconstruct musical information algorithmically; see the second figure below. 4) sheet music is incredibly dense in meaningful information compared to most computer vision problems, which means pipelines that have worked on those problems often do not transfer well to this task.

![triple](../assets/Schubert_TripletEx.png "Triplet")
*The symbol "3" appears in 8 different instances and 3 different contexts in these short three bars: as a time signature, as a fingering mark, and as a triplet marking.*

![triple](../assets/Sorabji_Slur+Ctxt.png)
*Extraordinary rule-breaking in music notation. A well-trained human player would be able to read and understand this almost immediately, whereas it is nearly impossible for current computer systems.
Source: [Donald Byrd](https://homes.luddy.indiana.edu/donbyrd/InterestingMusicNotation.html).*

As such, modern approaches in this field have rapidly shifted towards deep learning and neural networks to remove the complex intermediate steps in the OMR pipeline and instead treat it as an E2E (end-to-end) problem. Surprisingly, despite the inherent complexities of the problem, relatively simple networks have quickly outpaced traditional approaches in OMR. However, scores of complexity. One of the most complex forms of musical notation is pianoform; that is, two or more staves that may contain any number of voices in each, each of which may play any number of notes concurrently (in contrast, the simplest form of score, the monophonic score, consists of a single staff with one voice which only plays one note at once). In a recent publication *End-to-end optical music recognition for pianoform sheet music* (Ríos-Vila, et al., 2023), several end-to-end models are tested on pianoform scores, achieving a 3.9% error rate in the best models. However, I claim that there are several steps of music information retrieval that can still be performed completely algorithmically, which is not accounted for in this work. Notably, the model in the paper also attempts to extract semantic information such as the pitch of a note, which not only requires the model to know the location of the note on the page but also remember the current key signature and clef of the staff. This means that the model must learn ways to short-term and long-term information, something which may reduce potential accuracy of a smaller model. Indeed, the paper finds that a attention-based transformer model produces the most accurate results. However, transformers have a high computational cost that make them undesirable for deployment in practice on large sheet music images.

We investigate this problem by modifying the dataset such that the ground truths have been stripped of semantic information, and instead only contain visual information such as type and location. Such a ground-truth encoding can be called an **agnostic encoding** of the music, as opposed to a **semantic encoding** (Calvo-Zaragoza et al., 2018). An agnostic encoding can be obtained retrieving the *visual* information of the sheet music, that is, noting down that there is a note on the 5th line of the staff, and the clef and key signature, but not providing contextually implied information such as the explicit pitch of the note (leaving this to be figured out by a separate program, algorithmically according to the above steps). Thus, our approach will involve the complete removal of this semantic information from the dataset, instead leaving the *visual information only*. This way, our trained network can focus entirely on the extraction of local visual components on the score, which should hopefully improve performance. 

At this point, we should mention the principal challenge in completing this project: the scale of the data and model. `GRANDSTAFF` is a large dataset, containing thousands of images most of which have dimensions > 1000 pixels, and a correspondingly large number of weights to be updated in each pass (over 2 GiB per image). At the same time, it was impossible to downscale the images in preprocessing by any significant amount without losing meaningful visual information from the score entirely. Similarly, the model had to remain large and expressive to deal with the length of token sequences passed in with each input. With the GPU resources I was able to acquire, it was not possible to train a single epoch on 10% of the dataset. As it was infeasible to train a model to directly compete with the final results from (Rios Vila et al., 2023) with the computational resources available to me, I tried many approaches, including rescaling the images (as already discussed), reducing the training parameters, and reducing the number of weights in the model. Ultimately, the final approach was operating on a small fraction of the dataset (approximately 2%, which still left hundreds of thousands of tokens to train on). In order to have a direct point of comparison, I trained a version of the model on the original data alongside a model on the altered data.

## Data
For this experiment, we used the `GRANDSTAFF` dataset, which contains 53,882 single-system pianoform[^1] scores in common western modern notation. This is the dataset compiled and used in (Ríos-Vila et al., 2023), which makes it a good point of comparison for our proposed improvement to their original model. In addition, due to the lack of high-quality data in this field, there is a lack of other datasets which provides data for training and testing end-to-end networks on the OMR task for pianoform scores. Thus, `GRANDSTAFF` is possibly the only dataset which is suitable for the task. The data is publicly available at https://sites.google.com/view/multiscore-project/datasets.

However, the original partition setup is not publicly available (the original partitions are *not* random), so we use a randomly generated 80-10-10 train/validation/test split. The code to generate the partitions as well as the final partitions used are included in the code directory. 

## Methodology

The primary subject of investigation was whether or not the agnostic encoding would show improvement in the E2E model over the semantic encoding. Thus, we first reduce each semantic encoding to an equivalent agnostic encoding. Specifically, the original scores in the `GRANDSTAFF` dataset are provided in `**bekern` format, which contains non-visual information such as explicit pitch. Additionally, certain symbols such as key signatures and time-signatures are encoded per-voice rather than per-staff, meaning that the ground truth encoding often contains more of these symbols than are actually present on the page. Hence, we modify each of the ground truths to a new format, which we call `**vekern`. We then train the network to predict these new `**vekern` ground truths as a proxy for the original `**bekern`[^2].

The major change toward accomplishing an agnostic encoding is this: whereas in `**bekern`, every instance of a note is encoded by its explicit pitch and accidental, in `**vekern`, a note is encoded only by its relative distance from the middle line of the staff. Furthermore, an accidental is only included if it is visually present in the score. The pitch information can then be reconstructed from the information given in the latter, sparser encoding. The below figure shows an example of this. In addition to this the new encoding makes several other smaller changes following the same philosophy, the details of which can be found in `vekern.py`.

![](../assets/omr.png "bekern vs vekern")
*The same music encoded in `**bekern` (left) and `**vekern` (right). Notice how `**bekern` contains information which is not visually present in the score, such as the # next to the F which is implied from the key signature, whereas this information is removed in `**vekern`.*

The modified data is then passed through a deep neural network, in (ground truth, image) pairs. The specific architecture that we use is an encoder-decoder model, with the bulk of the encoder being a standard CNN (convolutional neural network) to transform the image of dimensions $(c, h, w)$ into a feature map of $(512, \frac{w}{16}, \frac{h}{8})$ through several layers of image convolution. Furthermore, this feature map is reshaped into a vector of dimension $(512, \frac{w}{16}\cdot \frac{h}{8})$, concatenating vertical slices of the feature map so as to preserve groupings of relevant features in sheet music together (since elements in sheet music occur horizontally, we want to keep elements appearing at the same vertical position together to preserve this spatial information). The feature vector is then passed into a decoder, which consists of a bidirectional LSTM (BLSTM) component and a linear layer to the final vocabulary output. A BLSTM consists of a sequence of RNN (recurrent neural network) blocks that have connections both to other RNN blocks as well as from and to the input and output. Using an RNN-based architecture has several advantages, the most important of which is being able to store both long-term and short-term information, making it suitable for sequence-based tasks such as NLP and OMR.

Following (Rios Vila et al., 2023), the model is trained with Connectionist Temporal Classification (CTC) loss with the Adam optimizer.

## Results
This model is evaluated on three metrics: character error rate (CER), symbol error rate (SER), and line error rate (LER).

CER refers to the edit distance between the ground-truth encoding and the model prediction, in terms of characters, or tokens (such as `N2`). SER refers to the the edit distance between complete musical symbols, which may be made up of multiple characters, according to a fixed grammar (such as `8 N2 -`). As, according to the grammar used by the network in the paper, complete musical notation symbols are represented by a collection of multiple characters, SER can be understood similarly to Word Error Rate in the OCR (Optical Character Recognition). Finally, LER refers to the amount of error in complete lines between the ground truth and prediction, which makes it useful to see if complete vertical slices of the information are being recalled accurately by the model, since the image-level token is constructed by concatenating these vertical slices.

At the end of 45 epochs of training, the `**vekern`-trained model beats the equivalent `**bekern`-trained model **in all three metrics** by a wide margin.

| **Encoding** | **CER**    | **LER**    | **SER**    |
|--------------|------------|------------|------------|
| `**vekern`   | **18.314** | **57.093** | **28.126** |
| `**bekern`   | 26.520     | 75.0       | 41.156     |

As a bonus, we also compare with a baseline model from the literature: a FCN (Fully Convolutional Network) trained on the **full** `GRANDSTAFF` dataset in the base `**kern` encoding (Rios Vila et al., 2023):

| **Model** | **CER**    | **LER**    | **SER**    |
|--------------|------------|------------|------------|
| CRNN, `**vekern`   | 18.314 | **57.093** | 28.126 |
| FCN, `**kern` | **14.3**     | 67.9       | **23.9**     |

We do astoundingly well against a model trained on the full dataset on a data-hungry problem. Our LER even beats this model by a wide margin. This suggests that our model performs strongly in the line prediction task. One possible explanation of this is that by alleviating the need of the model's attention/RNN-based components to learn context-based componenets of individual character encoding, the model is able to better learn line-wide semantic features using the same pieces of the architecture.

![vekern](../assets/vkrn.png "**vekern results")
*Prediction and ground truth labels from an excerpt predicted by the model trained on `**vekern`. Notice it is quite accurate in overall structure and note identification, and performs robustly on accidentals.*

![bekern](../assets/bkern.png "**bekern results")
*Similar except for `**bekern`. While it has an elementary grasp of the overall token grammar, it struggles with identification of notes significantly compared to the earlier model, and fails to identify any accidentals.*

Samples with common problems are visualized above, with `wandb`.

Also, the training of the model on the two datasets is visualized below. Notice the trend that `**vekern` initially learns faster than the alternative, and then continues to learn at a similar or greater pace. The two lines do not seem to converge; that is, `**bekern` does not continue learning, and it slows down as `**vekern` slows down. As both models move into overfitting area in the higher epochs as the validation SERs trend lower than the test set, further statements cannot be made without training these models on larger datasets. However, I skeptically speculate that training on `**vekern` is a strict computational improvement for the model as compared to training on `**bekern`, and the model will continue to outperform the original for an unbounded number of epochs over arbitrarily large amounts of training data.

![training](../assets/chart.png "training trends over epochs")

Some further areas of discussion area are as follows. The argument could be made that these models can learn rules of arbitrary complexity. However, models in this field of research are currently bounded by the amount of training data available, and hence rarer occurrences of these rules are extremely hard to learn. Removal of the semantic information simplifies the problem space greatly, allowing for models to improve quickly on basic tasks, as seen in our results.

## References
Calvo-Zaragoza, Jorge and David Rizo. “Camera-PrIMuS: Neural End-to-End Optical Music Recognition on Realistic Monophonic Scores.” *International Society for Music Information Retrieval Conference* (2018).

Ríos-Vila, A., Rizo, D., Iñesta, J.M. et al. End-to-end optical music recognition for pianoform sheet music. *IJDAR* 26, 347–362 (2023). https://doi.org/10.1007/s10032-023-00432-z

[^1]: **Pianoform** refers to a mode of sheet music with at least two staves, multiple voices per staff, and multiple notes per voice, making it the most complex mode of sheet music. Previous research has mostly focused on simpler modes of sheet music, such as monophonic scores, with one single-note voice on one staff, or homophony, with one multiple-note voice on one staff.
[^2]: Backward translation from `**vekern` to `**bekern` is not possible and therefore not built into the model, since there are sometimes multiple valid `**bekern` encodings of a single visual output. `**vekern` aims to eliminate these duplicates. However, conversion should be lossless with respect to the actual images, meaning that the image corresponding to the ground truth could be reproduced with the `**vekern` GT alone.